# Multi-Model Testing Configuration
#
# This file defines the LLM models to test in multi-model comparison mode.
# Each model will be evaluated across three modes: vanilla, web, and MARRVEL-MCP.
#
# Usage:
#   python evaluate_mcp.py --multi-model
#
# The result will show a grid comparing all models across all modes.

models:
  # OpenAI models
  - name: "GPT-4 Turbo"
    id: "openai/gpt-4-turbo"
    enabled: true
    description: "OpenAI GPT-4 Turbo - high capability model"

  - name: "GPT-3.5 Turbo"
    id: "openai/gpt-3.5-turbo"
    enabled: true
    description: "OpenAI GPT-3.5 Turbo - fast and cost-effective"

  # Anthropic Claude models
  - name: "Claude 3.5 Sonnet"
    id: "anthropic/claude-3.5-sonnet"
    enabled: true
    description: "Anthropic Claude 3.5 Sonnet - balanced performance"

  - name: "Claude 3 Haiku"
    id: "anthropic/claude-3-haiku"
    enabled: false
    description: "Anthropic Claude 3 Haiku - fast and efficient"

  # Google models
  - name: "Gemini 2.5 Flash"
    id: "google/gemini-2.5-flash"
    enabled: true
    description: "Google Gemini 2.5 Flash - default model (fast, with tool support)"

  - name: "Gemini 2.5 Pro"
    id: "google/gemini-2.5-pro"
    enabled: false
    description: "Google Gemini 2.5 Pro - high capability model"

  # Meta models
  - name: "Llama 3.3 70B"
    id: "meta-llama/llama-3.3-70b-instruct"
    enabled: false
    description: "Meta Llama 3.3 70B - open source model"

# Configuration options
config:
  # Only test models where enabled: true
  only_enabled: true

  # Timeout per model test (seconds)
  timeout: 300

  # Whether to cache results per model
  use_cache: true
